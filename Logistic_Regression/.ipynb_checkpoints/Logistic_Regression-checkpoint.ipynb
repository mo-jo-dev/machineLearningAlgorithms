{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = list(reader)\n",
    "        columns = data[0]\n",
    "        data = data[1:]\n",
    "        return data, columns\n",
    "\n",
    "data, columns = read_csv('./diabetes.csv')\n",
    "print(columns)\n",
    "for row in data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575448d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splitter(X, y, test_size = 0.2):\n",
    "    indices = list(range(len(X)))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    test_set_size = int(len(X) * test_size)\n",
    "    \n",
    "    test_indices = indices[:test_set_size]\n",
    "    train_indices = indices[test_set_size:]\n",
    "    \n",
    "    X_train = np.array([X[i] for i in train_indices])\n",
    "    X_test = np.array([X[i] for i in test_indices])\n",
    "    y_train = np.array([y[i] for i in train_indices])\n",
    "    y_test = np.array([y[i] for i in test_indices])\n",
    "    \n",
    "    return X_test, y_train, X_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e82213",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [row[:-1] for row in data]  # All columns except the last one\n",
    "y = [row[-1] for row in data]   # Only the last column\n",
    "\n",
    "print(\"First few rows of X before conversion:\", X[:5])\n",
    "print(\"First few rows of y before conversion:\", y[:5])\n",
    "\n",
    "X = [[float(value) for value in row] for row in X]\n",
    "y = [float(value) for value in y]\n",
    "\n",
    "print(\"First few rows of X after conversion:\", X[:5])\n",
    "print(\"First few rows of y after conversion:\", y[:5])\n",
    "\n",
    "X_test, y_train, X_train, y_test = train_test_splitter(X,y,1/3)\n",
    "\n",
    "print(\"X_test:\", X_test)\n",
    "print(\"y_test:\", y_test)\n",
    "print(\"X_train:\", X_train)\n",
    "np.set_printoptions(suppress=True)  # Suppress scientific notation for small numbers\n",
    "print(\"X_train after suppression:\",X_train)\n",
    "print(\"y_train:\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a63cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "class Logistic_Regression():\n",
    "    def __init__(self,learning_rate,iterations):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        \n",
    "    #Function for model training\n",
    "    def fit(self, X, Y):\n",
    "        # no_of_training_examples, no_of_features\n",
    "        self.m, self.n = X.shape\n",
    "        #weight initialization\n",
    "        self.W = np.zeros(self.n)\n",
    "        self.b = 0\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "        #gradient descent learning\n",
    "        for i in range(self.iterations):\n",
    "            self.update_weights()\n",
    "        return self\n",
    "\n",
    "    #Helper functions to update weights in gradient descent\n",
    "    \n",
    "    def update_weights(self):\n",
    "        A = 1/(1 + np.exp(-(self.X.dot(self.W) + self.b)))\n",
    "        \n",
    "        #claculate gradients\n",
    "        # slf.Y.T => transpose of Y\n",
    "        #self.m => no. of training examples\n",
    "        tmp = (A - self.Y.T) #diff between predicted prob. A and actual labels\n",
    "        tmp = np.reshape(tmp,self.m) # reshape tmp to shape determined by self.m\n",
    "        dW = np.dot(self.X.T, tmp)/self.m # gradient of weights by dot prod b/w transpose of X and tmp\n",
    "        db = np.sum(tmp) / self.m # gradient of bias (db) by taking the sum of all elements in tmp \n",
    "                                  # and then dividing by self.m, the number of training examples\n",
    "        \n",
    "        self.W = self.W - self.learning_rate*dW\n",
    "        self.b = self.b - self.learning_rate*db\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Z = 1/(1 + np.exp(- (X.dot(self.W) + self.b)))\n",
    "        Y = np.where(Z>0.5,1,0)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Logistic_Regression(learning_rate=0.01, iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "correctly_classified = 0\n",
    "\n",
    "count = 0\n",
    "for count in range(np.size(Y_pred)):\n",
    "\n",
    "    if y_test[count] == Y_pred[count]:\n",
    "        correctly_classified += 1\n",
    "\n",
    "    count += 1\n",
    "\n",
    "print( \"Accuracy on test set by our model       :  \", (  \n",
    "  correctly_classified / count ) * 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eedb855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
