import numpy as np
import matplotlib.pyplot as plt
import random
import csv
def read_csv(file_path):
    with open(file_path, 'r') as file:
        reader = csv.reader(file)
        data = list(reader)
        columns = data[0]
        data = data[1:]
        return data, columns

data, columns = read_csv('./diabetes.csv')
print(columns)
for row in data:
    print(row)
def train_test_splitter(X, y, test_size = 0.2):
    indices = list(range(len(X)))
    random.shuffle(indices)
    
    test_set_size = int(len(X) * test_size)
    
    test_indices = indices[:test_set_size]
    train_indices = indices[test_set_size:]
    
    X_train = np.array([X[i] for i in train_indices])
    X_test = np.array([X[i] for i in test_indices])
    y_train = np.array([y[i] for i in train_indices])
    y_test = np.array([y[i] for i in test_indices])
    
    return X_test, y_train, X_train, y_test
X = [row[:-1] for row in data]  # All columns except the last one
y = [row[-1] for row in data]   # Only the last column

print("First few rows of X before conversion:", X[:5])
print("First few rows of y before conversion:", y[:5])

X = [[float(value) for value in row] for row in X]
y = [float(value) for value in y]

print("First few rows of X after conversion:", X[:5])
print("First few rows of y after conversion:", y[:5])

X_test, y_train, X_train, y_test = train_test_splitter(X,y,1/3)

print("X_test:", X_test)
print("y_test:", y_test)
print("X_train:", X_train)
np.set_printoptions(suppress=True)  # Suppress scientific notation for small numbers
print("X_train after suppression:",X_train)
print("y_train:", y_train)
#Logistic Regression
class Logistic_Regression():
    def __init__(self,learning_rate,iterations):
        self.learning_rate = learning_rate
        self.iterations = iterations
        
    #Function for model training
    def fit(self, X, Y):
        # no_of_training_examples, no_of_features
        self.m, self.n = X.shape
        #weight initialization
        self.W = np.zeros(self.n)
        self.b = 0
        self.X = X
        self.Y = Y
        
        #gradient descent learning
        for i in range(self.iterations):
            self.update_weights()
        return self

    #Helper functions to update weights in gradient descent
    
    def update_weights(self):
        A = 1/(1 + np.exp(-(self.X.dot(self.W) + self.b)))
        
        #claculate gradients
        # slf.Y.T => transpose of Y
        #self.m => no. of training examples
        tmp = (A - self.Y.T) #diff between predicted prob. A and actual labels
        tmp = np.reshape(tmp,self.m) # reshape tmp to shape determined by self.m
        dW = np.dot(self.X.T, tmp)/self.m # gradient of weights by dot prod b/w transpose of X and tmp
        db = np.sum(tmp) / self.m # gradient of bias (db) by taking the sum of all elements in tmp 
                                  # and then dividing by self.m, the number of training examples
        
        self.W = self.W - self.learning_rate*dW
        self.b = self.b - self.learning_rate*db
        
        return self
    
    def predict(self, X):
        Z = 1/(1 + np.exp(- (X.dot(self.W) + self.b)))
        Y = np.where(Z>0.5,1,0)
        return Y
model = Logistic_Regression(learning_rate=0.01, iterations=1000)
model.fit(X_train, y_train)
Y_pred = model.predict(X_test)
correctly_classified = 0

count = 0
for count in range(np.size(Y_pred)):

    if y_test[count] == Y_pred[count]:
        correctly_classified += 1

    count += 1

print( "Accuracy on test set by our model       :  ", (  
  correctly_classified / count ) * 100 )
